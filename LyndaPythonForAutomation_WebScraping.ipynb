{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lynda - Python for Automation - Web Scraping\n",
    "\n",
    "* Course Home \n",
    "    * https://www.linkedin.com/learning/using-python-for-automation\n",
    "* Web Scraping Section\n",
    "    * https://www.linkedin.com/learning/using-python-for-automation/the-value-of-web-scraping\n",
    "    \n",
    "**Additional Resources:**\n",
    "* https://towardsdatascience.com/data-science-skills-web-scraping-using-python-d1a85ef607ed\n",
    "* https://learn.datacamp.com/courses/web-scraping-with-python\n",
    "* http://www.pythonforbeginners.com/beautifulsoup/beautifulsoup-4-python\n",
    "* https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "* https://doc.scrapy.org/en/latest/intro/tutorial.html\n",
    "    * https://scrapy.org/\n",
    "* file:///C:/Users/jpkee/Desktop/PythonProjects/Python%20Cheat%20Sheets/web%20scrapping.pdf\n",
    "* https://medium.com/analytics-vidhya/web-scraping-wiki-tables-using-beautifulsoup-and-python-6b9ea26d8722\n",
    "* https://www.youtube.com/watch?v=87Gx3U0BDlo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 3**\n",
    "\n",
    "Basic Steps\n",
    "1. Send a GET query to the website\n",
    "2. SAVE HTML-based doc is returned\n",
    "3. Parse the returned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need these libraries:\n",
    "1. Beautiful Soup\n",
    "    * Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work.\n",
    "2. lxml\n",
    "    * lxml is the most feature-rich and easy-to-use library for processing XML and HTML in the Python language.\n",
    "    * https://lxml.de/ \n",
    "3. requests\n",
    "    * Requests is an elegant and simple HTTP library for Python, built for human beings.\n",
    "    * https://requests.readthedocs.io/en/master/user/quickstart/\n",
    "4. urllib2 \n",
    "    * is a Python module for fetching URLs (Uniform Resource Locators). It offers a very simple    interface, in the form of the urlopen function.\n",
    "    * https://docs.python.org/2/library/urllib2.html\n",
    "    \n",
    "5. And we'll use this site for practice:\n",
    "    * http://quotes.toscrape.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some More Details**\n",
    "https://www.pythonforbeginners.com/beautifulsoup/beautifulsoup-4-python\n",
    "\n",
    "Grab a string, find all the b tags:\n",
    "* soup.find_all('b')\n",
    "\n",
    "A list, find all the x and y tags:\n",
    "* print soup.find_all([\"x\", \"y\"])\n",
    "\n",
    "\n",
    "\n",
    "Find_all method - most common methods in beautiful soup\n",
    "*examples of find_all\n",
    "1. soup.find_all('title')\n",
    "2. soup.find_all('p', 'title')\n",
    "3. soup.find_all('a')\n",
    "4. soup.find_all(id='link4')\n",
    "\n",
    "\n",
    "Navigation\n",
    "1. https://www.crummy.com/software/BeautifulSoup/bs4/doc/#navigating-the-tree\n",
    "2. https://www.pythonforbeginners.com/beautifulsoup/beautifulsoup-4-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good\n"
     ]
    }
   ],
   "source": [
    "# import the libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# some logging here for fun\n",
    "# import logging\n",
    "# logging.warning('\\nThis is the logging.warning bit')  # will print a message to the console\n",
    "# logging.info('I told you so')  # will not print anything\n",
    "\n",
    "# create a variable for the site\n",
    "url = 'http://quotes.toscrape.com'\n",
    "\n",
    "# create the request, response will get you the HTTP code\n",
    "response = requests.get(url)\n",
    "# if response == '200':\n",
    "#     print(response)\n",
    "# else:\n",
    "#     print('not gettinging')\n",
    "# 'response.text' returns the content of the response\n",
    "    # basically response just returns the content\n",
    "# we'll include the lxml parser\n",
    "# soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# let's see if it worked correctly by printing soup\n",
    "# print(soup)\n",
    "\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print('looks good')   \n",
    "else:\n",
    "    print('looks bad') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to navigate the parse tree is to say the name of the tag you want. \n",
    "If you want the <head> tag, just say:\n",
    "    \n",
    "    soup.head:\n",
    "or\n",
    "    \n",
    "    soup.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "repr(object)\n",
    "Returns a string containing a printable representation of an object.\n",
    "This is the same value yielded by conversions (reverse quotes). \n",
    "It is sometimes useful to be able to access this operation as an ordinary function. \n",
    "For many types, this function makes an attempt to return a string that would yield an object with the same value when passed to eval(), \n",
    "otherwise the representation is a string enclosed in angle brackets that contains the name of the type of the object together with additional information often including the name and address of the object. \n",
    "A class can control what this function returns for its instances by defining a __repr__() method.\n",
    "\n",
    "\n",
    ">>> s = \"String:\\tA\"\n",
    ">>> print s.encode('string_escape')\n",
    "String:\\tA\n",
    ">>> print repr(s)\n",
    "'String:\\tA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# loop over the strings\n",
    "for string in soup.strings:\n",
    "    print(repr(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More on response\n",
    "import requests \n",
    "\n",
    "# Making a get request \n",
    "response = requests.get('https://api.github.com') \n",
    "  \n",
    "# printing request text \n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find all the b tags\n",
    "# # soup.find_all('b')\n",
    "# # so can I find all trs?\n",
    "\n",
    "# this grabbed all of my column headers\n",
    "soup.find_all('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findin all anchor tags\n",
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.json will give you nicer output\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# r = requests.get('https://api.github.com/events')\n",
    "# r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so far we have ALL the code, but now to do we grab just what we want?\n",
    "# right click on any code and inspect it\n",
    "# <span class=\"text\"... has all of our bits so will be good to use, in the body tag, Within the div and body tags\n",
    "\n",
    "# reminder, HTML, Head and Body are the big tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# create a variable for the site\n",
    "url = 'http://quotes.toscrape.com'\n",
    "\n",
    "# create the request\n",
    "response = requests.get(url)\n",
    "\n",
    "# we'll include the lxml parser\n",
    "soup = bs(response.text, 'lxml')\n",
    "\n",
    "# create a variable for the quotes\n",
    "    # and use the find_all function\n",
    "quotes = soup.find_all('span', class_='text')\n",
    "# this above will work, but still grab some extra html\n",
    "\n",
    "# so let's create a loop to print each quote\n",
    "for quote in quotes:\n",
    "    print(quote.text +'\\n')\n",
    "\n",
    "# print(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's grab all the authors\n",
    "# these live in the small tag and author class \n",
    "\n",
    "\n",
    "# import the libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# create a variable for the site\n",
    "url = 'http://quotes.toscrape.com'\n",
    "\n",
    "# create the request\n",
    "response = requests.get(url)\n",
    "\n",
    "# we'll include the lxml parser\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# create a variable for the quotes\n",
    "# and use the find_all function\n",
    "quotes = soup.find_all('span', class_='text')\n",
    "# authors = soup.find_all('small', class_='author')\n",
    "for quote in quotes:\n",
    "    print(quote.text +'\\n')\n",
    "# for author in authors:\n",
    "#     print(author.text)\n",
    "\n",
    "# print(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = soup.find_all('small', class_='author')\n",
    "\n",
    "for author in authors:\n",
    "    print(author.text +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now update the for loop to print the authors and quotes\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://quotes.toscrape.com'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "quotes = soup.find_all('span', class_='text')\n",
    "authors = soup.find_all('small', class_='author')\n",
    "\n",
    "# so use the range function with the length of the quotes variable to stick them together\n",
    "for i in range(0, len(quotes)):\n",
    "    print(authors[i].text +':')\n",
    "    print(quotes[i].text+'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now let's get the corresponding tags ('deep-thoughts', 'change', etc)\n",
    "# # but that's using class 'tag', but there is >1 tag per quote so...\n",
    "# # go a step broader \n",
    "# # let's grab the div tag and class tag section\n",
    "# # each quote only has one tags section\n",
    "# # add this line to get the tags\n",
    "# tags = soup.find_all('div', class_=tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# url = 'http://quotes.toscrape.com'\n",
    "\n",
    "# response = requests.get(url)\n",
    "# soup = BeautifulSoup(response.text, 'lxml')\n",
    "# quotes = soup.find_all('span', class_='text')\n",
    "# authors = soup.find_all('small', class_='author')\n",
    "# # add this line to get the tags\n",
    "# tags = soup.find_all('div', class_='tags')\n",
    "\n",
    "# for i in range(0, len(quotes)):\n",
    "#     print(authors[i].text +':')\n",
    "#     print(quotes[i].text)\n",
    "#     print(tags[i].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That above works, but his solution was like this:\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://quotes.toscrape.com'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "quotes = soup.find_all('span', class_='text')\n",
    "authors = soup.find_all('small', class_='author')\n",
    "# add this line to get the tags\n",
    "tags = soup.find_all('div', class_='tags')\n",
    "\n",
    "for i in range(0, len(quotes)):\n",
    "    print(authors[i].text +':')\n",
    "    print(quotes[i].text)\n",
    "    quoteTags = tags[i].find_all('a', class_='tag')\n",
    "    # iterate thru all the quote tags and print the attributes\n",
    "    print('  Tags:')\n",
    "    for quoteTag in quoteTags:\n",
    "                print('   ' + quoteTag.text)\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the HTML\n",
    "#     <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
    "\n",
    "#         <span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>\n",
    "#         <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
    "#         <a href=\"/author/Albert-Einstein\">(about)</a>\n",
    "#         </span>\n",
    "#         <div class=\"tags\">\n",
    "#             Tags:\n",
    "#             <meta class=\"keywords\" itemprop=\"keywords\" content=\"change,deep-thoughts,thinking,world\" /    >      \n",
    "#             <a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n",
    "#             <a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n",
    "#             <a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n",
    "#             <a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\n",
    "#         </div>\n",
    "#     </div>\n",
    "\n",
    "# #  and here's how we match it up\n",
    "# this is the actual quote\n",
    "    # quotes = soup.find_all('span', class_='text')\n",
    "        ## <span class=\"text\" itemprop=\"text\">\n",
    "# this is the author\n",
    "    # authors = soup.find_all('small', class_='author')\n",
    "        ## <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
    "# here are the tags\n",
    "    # tags = soup.find_all('div', class_='tags')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm 200 Response\n",
    "# response\n",
    "# or print(response)\n",
    "\n",
    "# or even better, get the code\n",
    "# check for a 200\n",
    "  ##print(VaribleHere.status_code)\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('C:/Users/jpkee/Desktop/PythonProjects/Pictures/BeautifulSoupClass.JPG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing for paginated scraping**\n",
    "\n",
    "* https://www.linkedin.com/learning/using-python-for-automation/preparing-for-paginated-scraping\n",
    "* We'll use this site: https://scrapingclub.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this challenge: https://scrapingclub.com/exercise/list_basic\n",
    "# check out the pagination at the bottom\n",
    "\n",
    "# grab your modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# create request to the site, pointing to the specific page\n",
    "scrapySite = 'https://scrapingclub.com/exercise/list_basic/?page=1'\n",
    "response2 = requests.get(scrapySite)\n",
    "\n",
    "soup2 = BeautifulSoup(response2.text, 'lxml')\n",
    "items2 = soup2.find_all('div', class_='col-lg-4 col-md-6 mb-4')\n",
    "\n",
    "count = 1\n",
    "for i in items2:\n",
    "    itemName = i.find('h4', class_='card-title').text.strip('\\n')\n",
    "    itemPrice = i.find('h5').text\n",
    "    print('%s ) Price: %s, Item Name: %s' % (count, itemPrice, itemName))\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image('C:/Users/jpkee/Desktop/PythonProjects/Pictures/BeautifulSoupCardTitle.JPG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's add multipaging!\n",
    "# check out the href for the pages, page link etc\n",
    "        # hypertext reference ;)\n",
    "pages = soup2.find('ul', class_='pagination')\n",
    "urls = []\n",
    "links = pages.find_all('a', class_='page-link')\n",
    "\n",
    "# iterate thru all the page link elements\n",
    "for link in links:\n",
    "     # make sure the page number is a digit\n",
    "    pageNum = int(link.text) if link.text.isdigit() else None\n",
    "#     isDigit? https://www.tutorialspoint.com/python/string_isdigit.htm\n",
    "    #check if page number != none, if not, add it to the x\n",
    "    if pageNum != None:\n",
    "        x = link.get('href')\n",
    "        urls.append(x)\n",
    "        # you'll get ''?page=7' if you print x \n",
    "# print(urls)\n",
    "count = 1\n",
    "for i in urls:\n",
    "    newUrl = url + i\n",
    "    response = requests.get(newUrl)\n",
    "    soup2 = BeautifulSoup(response2.text, 'lxml')\n",
    "    items2 = soup2.find_all('div', class_='col-lg-4 col-md-6 mb-4')\n",
    "\n",
    "    for i in items2:\n",
    "        itemName = i.find('h4', class_='card-title').text.strip('\\n')\n",
    "        itemPrice = i.find('h5').text\n",
    "        # Just the formatting bit here\n",
    "        print('%s ) Price: %s, Item Name: %s' % (count, itemPrice, itemName))\n",
    "        count = count + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('C:/Users/jpkee/Desktop/PythonProjects/Pictures/BeautifulSoupCardPage.JPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Velodrome Info from Wikipedia**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now, try to get the velodrome details here: https://en.wikipedia.org/wiki/List_of_cycling_tracks_and_velodromes\n",
    "# # grab your modules\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import prettify\n",
    "\n",
    "# # create request to the site, pointing to the specific page\n",
    "# velodromes = 'https://en.wikipedia.org/wiki/List_of_cycling_tracks_and_velodromes'\n",
    "# veloResponse = requests.get(velodromes)\n",
    "# # Grab the page contents\n",
    "# tracks = BeautifulSoup(veloResponse.text, 'lxml')\n",
    "\n",
    "# # So the tables are in wikitable sortable\n",
    "# # Per https://medium.com/analytics-vidhya/web-scraping-wiki-tables-using-beautifulsoup-and-python-6b9ea26d8722\n",
    "# #     # we need to put this into a dictionary\n",
    "    \n",
    "\n",
    "# # find the table\n",
    "# veloTable = tracks.find('table', {'class': 'wikitable sortable'})\n",
    "# # find all the anchors\n",
    "# veloLinks = veloTable.find_all('a')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # # I want all the track names so, extract the track names into a list\n",
    "# # # Create the empty list\n",
    "# # trackNames = []\n",
    "# # # Loop thru the cells and grab all the names, putting them into the empty 'trackNames' list\n",
    "\n",
    "\n",
    "\n",
    "# # <span class=\"mw-headline\" id=\"Velodromes_currently_in_use\">Velodromes currently in use</span>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(tracks.prettify())\n",
    "# # print(veloTable)\n",
    "# print(veloLinks)\n",
    "# # print(veloLinks.prettify())\n",
    "# # veloLinks()\n",
    "# # print(veloLinks.prettify())\n",
    "# # print(tracks)\n",
    "# # https://medium.com/analytics-vidhya/web-scraping-wiki-tables-using-beautifulsoup-and-python-6b9ea26d8722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find vs find_all\n",
    "## per https://linuxhint.com/python-beautifulsoup-tutorial-for-beginners/#:~:text=The%20find%20method%20searches%20for,element.&text=The%20find_all%20method%20on%20the,a%20list%20of%20type%20bs4.\n",
    "\n",
    "# The find method searches for the first tag with the needed name and returns an object of type bs4.element.Tag.\n",
    "\n",
    "# The find_all method on the other hand, searches for all tags with the needed tag name and \n",
    "# returns them as a list of type bs4.element.ResultSet. All the items in the list are \n",
    "# of type bs4.element.Tag, so we can carry out indexing on the list and continue our beautifulsoup exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is cell 'A1' basically\n",
    "# <a href=\"/wiki/Argentina\" title=\"Argentina\">Argentina</a>\n",
    "\n",
    "# we'll include the lxml parser\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# create a variable for the quotes\n",
    "# and use the find_all function\n",
    "quotes = soup.find_all('span', class_='text')\n",
    "# this above will work, but still grab some extra html\n",
    "\n",
    "# so let's create a loop to print each quote\n",
    "for quote in quotes:\n",
    "    print(quote.text +'\\n')\n",
    "\n",
    "# print(quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More on Scraping Tools**\n",
    "1. Beautiful Soup\n",
    "2. Scrapy\n",
    "3. Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('C:/Users/jpkee/Desktop/PythonProjects/Pictures/BeautifulSoupVsSeleniumVsScrapy.JPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "source_code = requests.get('http://en.wikipedia.org/wiki/Taylor_Swift_discography')\n",
    "soup = BeautifulSoup(source_code.content)\n",
    "\n",
    "# grab the table\n",
    "table = soup.find('span', id='Singles').parent.find_next_sibling('table')\n",
    "\n",
    "# grab all the columns from that table\n",
    "for single in table.find_all('th', scope='col'):\n",
    "    print(single.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "source_code = requests.get('http://en.wikipedia.org/wiki/Taylor_Swift_discography')\n",
    "soup = BeautifulSoup(source_code.content)\n",
    "\n",
    "table = soup.find('span', id='Singles').parent.find_next_sibling('table')\n",
    "for single in table.find_all('th', scope='row'):\n",
    "    print(single.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try to get Taylor Swift's studio albums, which is a similar table structure I think\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "source_codeAlbum = requests.get('http://en.wikipedia.org/wiki/Taylor_Swift_discography')\n",
    "soupAlbum = BeautifulSoup(source_code.content)\n",
    "\n",
    "tableAlbum = soupAlbum.find('span', id='Studio_albums').parent.find_next_sibling('table')\n",
    "for single in tableAlbum.find_all('th', scope='row'):\n",
    "    print(single.text)\n",
    "    \n",
    "    \n",
    "# now how to get the 2nd column?\n",
    "tableDetails = soupAlbum.find('span', id='Album').parent.find_next_sibling('table')\n",
    "for single in tableDetails.find_all('th', scope='column'):\n",
    "    print(single.text)\n",
    "\n",
    "    \n",
    "# this is how it lines up\n",
    "    # quotes = soup.find_all('span', class_='text')\n",
    "        ##                   <span class=\"text\" itemprop=\"text\">\n",
    "        \n",
    "# so to get the 2nd column\n",
    "\n",
    "<th scope=\"col\" rowspan=\"2\" style=\"width:18em;\">Album details</th>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "website_url = requests.get('https://en.wikipedia.org/wiki/List_of_cycling_tracks_and_velodromes').text\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(website_url,'lxml')\n",
    "print(soup.prettify())\n",
    "\n",
    "My_table = soup.find('table',{'class':'wikitable sortable'})\n",
    "links = My_table.findAll('a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Countries = []\n",
    "for link in links:\n",
    "    Countries.append(link.get('title'))\n",
    "    \n",
    "print(Countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df['Country'] = Countries\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
