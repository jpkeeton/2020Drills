{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lynda - Python for Automation - Web Scraping\n",
    "\n",
    "* Course Home \n",
    "    * https://www.linkedin.com/learning/using-python-for-automation\n",
    "* Web Scraping Section\n",
    "    * https://www.linkedin.com/learning/using-python-for-automation/the-value-of-web-scraping\n",
    "    \n",
    "Additional Resources:\n",
    "* https://towardsdatascience.com/data-science-skills-web-scraping-using-python-d1a85ef607ed\n",
    "* https://campus.datacamp.com/courses/web-scraping-with-python/introduction-to-html?ex=1\n",
    "* http://www.pythonforbeginners.com/beautifulsoup/beautifulsoup-4-python\n",
    "* https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "* https://doc.scrapy.org/en/latest/intro/tutorial.html\n",
    "    * https://scrapy.org/\n",
    "* file:///C:/Users/jpkee/Desktop/PythonProjects/Python%20Cheat%20Sheets/web%20scrapping.pdf\n",
    "* https://medium.com/analytics-vidhya/web-scraping-wiki-tables-using-beautifulsoup-and-python-6b9ea26d8722\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 3**\n",
    "\n",
    "Basic Steps\n",
    "1. Send a GET query to the website\n",
    "2. SAVE HTML-based doc is returned\n",
    "3. Parse the returned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need these libraries:\n",
    "1. Beautiful Soup\n",
    "    * Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work.\n",
    "2. lxml\n",
    "    * lxml is the most feature-rich and easy-to-use library for processing XML and HTML in the Python language.\n",
    "    * https://lxml.de/ \n",
    "3. requests\n",
    "    * Requests is an elegant and simple HTTP library for Python, built for human beings.\n",
    "    * https://requests.readthedocs.io/en/master/user/quickstart/\n",
    "    \n",
    "And we'll use this site for practice:\n",
    "* http://quotes.toscrape.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# create a variable for the site\n",
    "url = 'http://quotes.toscrape.com'\n",
    "\n",
    "# create the request\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "# 'response.text' returns the content of the response\n",
    "    # basically response just returns the content\n",
    "# we'll include the lxml parser\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "\n",
    "\n",
    "# let's see if it worked correctly by printing soup\n",
    "# soup\n",
    "# or \n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More on response\n",
    "\n",
    "import requests \n",
    "  \n",
    "# Making a get request \n",
    "response = requests.get('https://api.github.com') \n",
    "  \n",
    "# printing request text \n",
    "print(response.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# r = requests.get('https://api.github.com/events')\n",
    "# r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so far we have ALL the code, but now to do we grab just what we want?\n",
    "# right click on any code and inspect it\n",
    "# <span class=\"text\"... has all of our bits so will be good to use, in the body tag, Within the div and body tags\n",
    "\n",
    "# reminder, HTML, Head and Body are the big tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# create a variable for the site\n",
    "url = 'http://quotes.toscrape.com'\n",
    "\n",
    "# create the request\n",
    "response = requests.get(url)\n",
    "\n",
    "# we'll include the lxml parser\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# create a variable for the quotes\n",
    "# and use the find_all function\n",
    "quotes = soup.find_all('span', class_='text')\n",
    "# this above will work, but still grab some extra html\n",
    "\n",
    "# so let's create a loop to print each quote\n",
    "for quote in quotes:\n",
    "    print(quote.text +'\\n')\n",
    "\n",
    "# print(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's grab all the authors\n",
    "# these live in the small tag and author class\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import the libraries\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# # create a variable for the site\n",
    "# url = 'http://quotes.toscrape.com'\n",
    "\n",
    "# # create the request\n",
    "# response = requests.get(url)\n",
    "\n",
    "# # we'll include the lxml parser\n",
    "# soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# # create a variable for the quotes\n",
    "# # and use the find_all function\n",
    "# quotes = soup.find_all('span', class_='text')\n",
    "# # authors = soup.find_all('small', class_='author')\n",
    "# for quote in quotes:\n",
    "#     print(quote.text +'\\n')\n",
    "\n",
    "# # print(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authors = soup.find_all('small', class_='author')\n",
    "\n",
    "# for author in authors:\n",
    "#     print(author.text +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # So now update the for loop to print the authors and quotes\n",
    "\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# url = 'http://quotes.toscrape.com'\n",
    "\n",
    "# response = requests.get(url)\n",
    "# soup = BeautifulSoup(response.text, 'lxml')\n",
    "# quotes = soup.find_all('span', class_='text')\n",
    "# authors = soup.find_all('small', class_='author')\n",
    "\n",
    "# # so use the range function with the length of the quotes variable to stick them together\n",
    "# for i in range(0, len(quotes)):\n",
    "#     print(authors[i].text +':')\n",
    "#     print(quotes[i].text+'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now let's get the corresponding tags ('deep-thoughts', 'change', etc)\n",
    "# # but that's using class 'tag', but there is >1 tag per quote so...\n",
    "# # go a step broader \n",
    "# # let's grab the div tag and class tag section\n",
    "# # each quote only has one tags section\n",
    "# # add this line to get the tags\n",
    "# tags = soup.find_all('div', class_=tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# url = 'http://quotes.toscrape.com'\n",
    "\n",
    "# response = requests.get(url)\n",
    "# soup = BeautifulSoup(response.text, 'lxml')\n",
    "# quotes = soup.find_all('span', class_='text')\n",
    "# authors = soup.find_all('small', class_='author')\n",
    "# # add this line to get the tags\n",
    "# tags = soup.find_all('div', class_='tags')\n",
    "\n",
    "# for i in range(0, len(quotes)):\n",
    "#     print(authors[i].text +':')\n",
    "#     print(quotes[i].text)\n",
    "#     print(tags[i].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That above works, but his solution was like this:\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://quotes.toscrape.com'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "quotes = soup.find_all('span', class_='text')\n",
    "authors = soup.find_all('small', class_='author')\n",
    "# add this line to get the tags\n",
    "tags = soup.find_all('div', class_='tags')\n",
    "\n",
    "for i in range(0, len(quotes)):\n",
    "    print(authors[i].text +':')\n",
    "    print(quotes[i].text)\n",
    "    quoteTags = tags[i].find_all('a', class_='tag')\n",
    "    # iterate thru all the quote tags and print the attributes\n",
    "    print('  Tags:')\n",
    "    for quoteTag in quoteTags:\n",
    "                print('   ' + quoteTag.text)\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the HTML\n",
    "#     <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
    "\n",
    "#         <span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>\n",
    "#         <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
    "#         <a href=\"/author/Albert-Einstein\">(about)</a>\n",
    "#         </span>\n",
    "#         <div class=\"tags\">\n",
    "#             Tags:\n",
    "#             <meta class=\"keywords\" itemprop=\"keywords\" content=\"change,deep-thoughts,thinking,world\" /    >      \n",
    "#             <a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n",
    "#             <a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n",
    "#             <a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n",
    "#             <a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\n",
    "#         </div>\n",
    "#     </div>\n",
    "\n",
    "# #  and here's how we match it up\n",
    "# this is the actual quote\n",
    "    # quotes = soup.find_all('span', class_='text')\n",
    "        ## <span class=\"text\" itemprop=\"text\">\n",
    "# this is the author\n",
    "    # authors = soup.find_all('small', class_='author')\n",
    "        ## <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
    "# here are the tags\n",
    "    # tags = soup.find_all('div', class_='tags')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm 200 Response\n",
    "response\n",
    "# or print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "# Image('C:/Users/jpkee/Desktop/PythonProjects/Pictures/BeautifulSoupClass.JPG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing for paginated scraping**\n",
    "\n",
    "* https://www.linkedin.com/learning/using-python-for-automation/preparing-for-paginated-scraping\n",
    "* We'll use this site: https://scrapingclub.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this challenge: https://scrapingclub.com/exercise/list_basic\n",
    "# check out the pagination at the bottom\n",
    "\n",
    "# grab your modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# create request to the site, pointing to the specific page\n",
    "scrapySite = 'https://scrapingclub.com/exercise/list_basic/?page=1'\n",
    "response2 = requests.get(scrapySite)\n",
    "\n",
    "soup2 = BeautifulSoup(response2.text, 'lxml')\n",
    "items2 = soup2.find_all('div', class_='col-lg-4 col-md-6 mb-4')\n",
    "\n",
    "count = 1\n",
    "for i in items2:\n",
    "    itemName = i.find('h4', class_='card-title').text.strip('\\n')\n",
    "    itemPrice = i.find('h5').text\n",
    "    print('%s ) Price: %s, Item Name: %s' % (count, itemPrice, itemName))\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image('C:/Users/jpkee/Desktop/PythonProjects/Pictures/BeautifulSoupCardTitle.JPG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now let's add multipaging!\n",
    "# # check out the href for the pages, page link etc\n",
    "#         # hypertext reference ;)\n",
    "# pages = soup2.find('ul', class_='pagination')\n",
    "# urls = []\n",
    "# links = pages.find_all('a', class_='page-link')\n",
    "\n",
    "# # iterate thru all the page links elements\n",
    "# for link in links:\n",
    "#     pageNum = int(link.text) if link.text.isdigit() else None\n",
    "# #     isDigit? https://www.tutorialspoint.com/python/string_isdigit.htm\n",
    "#     #check if page number != none\n",
    "#     if pageNum != None:\n",
    "#         x = link.get('href')\n",
    "#         urls.append(x)\n",
    "# # print(urls)\n",
    "# count = 1\n",
    "# for i in urls:\n",
    "#     newUrl = url + i\n",
    "#     response = requests.get(newUrl)\n",
    "#     soup2 = BeautifulSoup(response2.text, 'lxml')\n",
    "#     items2 = soup2.find_all('div', class_='col-lg-4 col-md-6 mb-4')\n",
    "\n",
    "#     for i in items2:\n",
    "#         itemName = i.find('h4', class_='card-title').text.strip('\\n')\n",
    "#         itemPrice = i.find('h5').text\n",
    "#         print('%s ) Price: %s, Item Name: %s' % (count, itemPrice, itemName))\n",
    "#         count = count + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('C:/Users/jpkee/Desktop/PythonProjects/Pictures/BeautifulSoupCardPage.JPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Velodrome Info from Wikipedia**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, try to get the velodrome details here: https://en.wikipedia.org/wiki/List_of_cycling_tracks_and_velodromes\n",
    "# grab your modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import prettify\n",
    "\n",
    "# create request to the site, pointing to the specific page\n",
    "velodromes = 'https://en.wikipedia.org/wiki/List_of_cycling_tracks_and_velodromes'\n",
    "veloResponse = requests.get(velodromes)\n",
    "# Grab the page contents\n",
    "tracks = BeautifulSoup(veloResponse.text, 'lxml')\n",
    "\n",
    "# So the tables are in wikitable sortable\n",
    "# Per https://medium.com/analytics-vidhya/web-scraping-wiki-tables-using-beautifulsoup-and-python-6b9ea26d8722\n",
    "#     # we need to put this into a dictionary\n",
    "    \n",
    "\n",
    "\n",
    "veloTable = tracks.find('table', {'class': 'wikitable sortable'})\n",
    "veloLinks = veloTable.find_all('a')\n",
    "\n",
    "# similar to this?: \n",
    "#     quotes = soup.find_all('span', class_='text')\n",
    "# class=\"wikitable sortable jquery-tablesorter\"\n",
    "\n",
    "\n",
    "# print(tracks.prettify())\n",
    "# print(veloTable)\n",
    "# print(veloLinks)\n",
    "# print(veloLinks.prettify())\n",
    "# veloLinks()\n",
    "# print(veloLinks.prettify())\n",
    "# print(tracks)\n",
    "# https://medium.com/analytics-vidhya/web-scraping-wiki-tables-using-beautifulsoup-and-python-6b9ea26d8722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find vs find_all\n",
    "## per https://linuxhint.com/python-beautifulsoup-tutorial-for-beginners/#:~:text=The%20find%20method%20searches%20for,element.&text=The%20find_all%20method%20on%20the,a%20list%20of%20type%20bs4.\n",
    "\n",
    "# The find method searches for the first tag with the needed name and returns an object of type bs4.element.Tag.\n",
    "\n",
    "# The find_all method on the other hand, searches for all tags with the needed tag name and \n",
    "# returns them as a list of type bs4.element.ResultSet. All the items in the list are \n",
    "# of type bs4.element.Tag, so we can carry out indexing on the list and continue our beautifulsoup exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is cell 'A1' basically\n",
    "# <a href=\"/wiki/Argentina\" title=\"Argentina\">Argentina</a>\n",
    "\n",
    "# we'll include the lxml parser\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# create a variable for the quotes\n",
    "# and use the find_all function\n",
    "quotes = soup.find_all('span', class_='text')\n",
    "# this above will work, but still grab some extra html\n",
    "\n",
    "# so let's create a loop to print each quote\n",
    "for quote in quotes:\n",
    "    print(quote.text +'\\n')\n",
    "\n",
    "# print(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
