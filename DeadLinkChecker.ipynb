{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dead Link Tester**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First challenge/project: \n",
    "\n",
    "1. User enters a web site address.\n",
    "    * JK - this is from the command line ya?\n",
    "        * JK - not a GUI? though that'd be cool\n",
    "2. Program accesses site and checks all page resources (e.g., js files, css files, images, etc.).\n",
    "    * JK - so page is launched and \n",
    "    * JK - starts looking or just HREFs or, actually evertyhing?\n",
    "* Program notes any that are \"dead.\"\n",
    "    * JK - like pgm will open a tab or window and check for 404?\n",
    "* Program follows href addresses on the page and repeats the test.\n",
    "    * JK - yah like above, should \n",
    "* Program stays in the same domain.\n",
    "    * JK - should be fine, don't leave\n",
    "* Program has a \"depth\" limit on following href-chains \n",
    "    * ** (e.g., don't go more than 10 pages \"deep\" into a site).\n",
    "* Program does not perform a repeat test of any page or link.\n",
    "    * JK - ok, not sure how that'd work\n",
    "* Program has a total pages test limit.\n",
    "    * JK - ok, not sure how that'd work\n",
    "* Program should have a time limit safety.\n",
    "    * JK - like if run time = > n seconds\n",
    "        * JK - break out and quit, close the application\n",
    "* Program should have a throttle setting (number of tests per second or similar).\n",
    "    * JK - something like a sleep/wait or whatever\n",
    "* Program should test javascript-generated content as well.\n",
    "    * JK - that can probably done with xpath\n",
    "* Final report should go to console and text file (plain text, but human readable).\n",
    "    * Report should show, for each page tested, the main URL, links on the page, total dead ones.\n",
    "    * Report should show the total test time.\n",
    "    * Program shows progress on console during testing.\n",
    "\n",
    "Use whatever common modules are needed to accomplish the task. Likely needs include Splinter or Selenium. Requests and BeautifulSoup might end up being useful, too.\n",
    "\n",
    "Optional: Create process such that the test is automatically executed once per day and sends an alert to an admin if any problems are encountered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do:\n",
    "* Study Splinter\n",
    "* \n",
    "* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splinter\n",
    "# https://www.youtube.com/watch?v=ApA7EVwSzg0\n",
    "# seems to be often used alongside Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at my other selenium and see if i'm pullin in the chromedriver\n",
    "# https://github.com/jpkeeton/2020Drills/blob/master/SeleniumStudies.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting test\n",
      "Kickin' off the test!\n",
      "Closing the Browser\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "# The Keys class provide keys in the keyboard like RETURN, F1, ALT etc.\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# pointing to the specific version here\n",
    "# so here, driver is a variable and the \n",
    "driver = webdriver.Chrome(executable_path=r'C:\\Users\\jpkee\\Downloads\\chromedriver_win32\\chromedriver.exe')\n",
    "print('starting test')\n",
    "\n",
    "\n",
    "# the driver.get method will navigate to a page given by the URL\n",
    "# Webdriver will wait unti lthe page has fully loaded, aka the 'onload' event has fired\n",
    "driver.get(\"http://www.python.org\")\n",
    "\n",
    "# just a print statement for fun/tracking\n",
    "print(\"Kickin' off the test!\")\n",
    "assert \"Python\" in driver.title\n",
    "\n",
    "# Close\n",
    "print('Closing the Browser')\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config Module\n",
    "* module for configuring Python programs \n",
    "* aims to offer more power and flexibility than existing ConfigParser module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test\n",
      "Maximizing window\n",
      "Ending test\n"
     ]
    }
   ],
   "source": [
    "# More Splinter Work\n",
    "# https://www.youtube.com/watch?v=ApA7EVwSzg0\n",
    "from splinter import Browser\n",
    "from selenium import webdriver\n",
    "# from config import username, password\n",
    "\n",
    "\n",
    "executable_path={'executable_path':r'C:\\Users\\jpkee\\Downloads\\chromedriver_win32\\chromedriver.exe'}\n",
    "# driver = webdriver.Chrome(\n",
    "print('Starting test')\n",
    "\n",
    "\n",
    "#set some default behaviors for browser\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# Maximize windows\n",
    "print('Maximizing window')\n",
    "options.add_argument(\"--start-maximized\")\n",
    "\n",
    "# Disable notifications\n",
    "options.add_argument(\"--disable-notifications\")\n",
    "\n",
    "# create the browser and call the ex. path variable\n",
    "# and we'll use headless = False so the brower will actually launch\n",
    "browser = Browser('chrome', **executable_path, headless=True)\n",
    "browser.visit(\"http://www.python.org\")\n",
    "\n",
    "# assert \"Python\" in browser.title\n",
    "\n",
    "searchy = \"Pythond\"\n",
    "if searchy in browser.title:\n",
    "    print('Found it')\n",
    "else:\n",
    "    ('Nope, not there')\n",
    "    \n",
    "print('Ending test')\n",
    "browser.quit()\n",
    "\n",
    "# Next Steps, point to a different site, twitter?\n",
    "# grab some fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all hrefs\n",
    "# from BeautifulSoup import BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'urllib' has no attribute 'urlopen'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-23375676651a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhtml_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"http://www.cnn.com\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml_page\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# # for link in soup.findAll('a'):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# #     print link.get('href')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'urllib' has no attribute 'urlopen'"
     ]
    }
   ],
   "source": [
    "html_page = urllib2.urlopen(\"http://www.cnn.com\")\n",
    "soup = BeautifulSoup(html_page)\n",
    "# # for link in soup.findAll('a'):\n",
    "# #     print link.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-88ca37d33e9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlinks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Company\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'soup' is not defined"
     ]
    }
   ],
   "source": [
    "links = soup.findAll('tr')[5].findAll('a', href=re.compile(\"Company\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
